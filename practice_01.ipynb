{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObUf5G9VfAU6W1I3RJCmol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MKJJang/airflow/blob/master/practice_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze3KkD6JlacW",
        "outputId": "df88ef20-a527-424c-93e4-24425b40504a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n"
          ]
        }
      ],
      "source": [
        "# 띄어쓰기 단위로 분리\n",
        "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
        "input_text_list = input_text.split()\n",
        "print(\"input_text_list: \", input_text_list)\n",
        "# input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
        "\n",
        "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str2idx = {word:idx for idx, word  in enumerate(input_text_list)}\n",
        "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
        "print(\"str2idx: \", str2idx)\n",
        "print(\"idx2str: \", idx2str)\n",
        "\n",
        "# 토큰을 토큰 아이디로 변환\n",
        "input_ids = [str2idx[word] for word in input_text_list]\n",
        "print(\"input_ids: \", input_ids)\n",
        "\n",
        "# str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
        "# idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
        "# input_ids:  [0, 1, 2, 3, 4]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2HsN87pmInw",
        "outputId": "4d113c0f-6752-45b0-c4e8-ed124c9fdebb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
            "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
            "input_ids:  [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 16\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)"
      ],
      "metadata": {
        "id": "Aoom8xiCpjYK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
        "input_embeddings = input_embeddings.unsqueeze(0)\n",
        "input_embeddings.shape\n",
        "input_embeddings = input_embeddings.unsqueeze(0)\n",
        "input_embeddings.shape\n",
        "\n",
        "# 1개의 문장이고 5개의 토큰이 있고 16차원의 임베딩이 생성되었음을 확인할 수 있다.\n",
        "# torch.Size([1, 1, 5, 16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF_4fO0AmIqs",
        "outputId": "92b10a75-fc36-4b0e-bb31-55f0a3bf81d5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_dim = 16\n",
        "max_position = 12\n",
        "embeded_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
        "\n",
        "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
        "position_encodings = position_embed_layer(position_ids)\n",
        "toked_embeddings = embed_layer(torch.tensor(input_ids)) # (5,16)\n",
        "token_embeddings = toked_embeddings.unsqueeze(0) # (1,5,16)\n",
        "token_embeddings = toked_embeddings + position_encodings\n",
        "input_embeddings.shape\n",
        "\n",
        "# torch.Size([1, 1, 5, 16])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P40RP4-ImItU",
        "outputId": "2a152ef8-d5ab-45f1-eef3-34aff5709a62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 2.4. 쿼리, 키, 값 벡터를 만드는 nn.Linear 층\n",
        "\n",
        "head_dim = 16\n",
        "\n",
        "#쿼리, 키, 값을 계산하기 위한 변환\n",
        "weight_q = nn.Linear(embedding_dim, head_dim)\n",
        "weight_k = nn.Linear(embedding_dim, head_dim)\n",
        "weight_v = nn.Linear(embedding_dim, head_dim)\n",
        "\n",
        "# 변환 수행\n",
        "querys = weight_q(token_embeddings) #(1,5,6)\n",
        "keys = weight_k(token_embeddings) #(1,5,6)\n",
        "values = weight_v(token_embeddings) #(1,5,6)\n",
        "\n",
        "print(querys)\n",
        "print(keys)\n",
        "print(values)"
      ],
      "metadata": {
        "id": "yhglxxsKmIv7",
        "outputId": "01afc24e-8cc6-4520-8eee-e8cc13a62754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.6162,  1.0827, -0.2643,  1.6744, -0.0235, -0.7401,  0.5347,\n",
            "          -0.2917, -0.1730, -1.6108, -0.4645,  0.4613,  0.6690,  0.6273,\n",
            "          -0.3162, -0.0382],\n",
            "         [-0.6820,  0.8528,  0.2134,  0.2797, -0.5687, -1.0554,  0.1513,\n",
            "          -1.6495,  0.2305, -2.6452, -0.0979,  1.2764, -0.0178,  0.5187,\n",
            "          -0.3336,  0.7411],\n",
            "         [ 0.4693, -1.3521, -1.0545, -0.2039,  0.6332, -0.2554,  0.1118,\n",
            "           0.1425, -0.2897, -0.0209,  0.0338,  0.4834,  0.1068,  0.0790,\n",
            "           0.1575, -0.7535],\n",
            "         [-0.9703,  0.9017, -0.5385,  1.2906,  1.1570, -0.2884, -1.4554,\n",
            "           0.6264,  1.0711, -2.2912, -1.1444,  0.7135,  0.1785,  1.0821,\n",
            "           0.4626, -0.3524],\n",
            "         [ 0.8193,  0.1722, -1.1576, -0.1091, -1.7512, -0.6975,  0.5769,\n",
            "           1.1210,  0.7730,  0.2605,  0.0962, -1.5100,  0.1112, -0.9559,\n",
            "           0.2537, -1.0503]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[-1.2399e+00, -1.9895e+00,  8.9470e-02, -1.3158e-01,  6.8075e-01,\n",
            "          -3.8095e-01, -5.1436e-02, -8.5357e-01, -3.5073e-01,  1.8276e+00,\n",
            "           1.9522e-01,  9.2040e-01,  1.0637e-02, -1.6252e+00, -7.6067e-01,\n",
            "           4.0486e-01],\n",
            "         [-6.2908e-01,  2.2806e-03, -5.6171e-01,  7.6563e-01,  9.1364e-01,\n",
            "          -4.5702e-01, -3.2257e-01,  7.6495e-01, -4.6453e-01,  6.9347e-01,\n",
            "           2.9678e-01, -3.7600e-01, -2.5620e-02, -7.5102e-01,  4.1220e-01,\n",
            "           5.2301e-02],\n",
            "         [-1.0747e-01, -6.7479e-01,  6.3563e-01, -1.0956e+00,  1.2500e+00,\n",
            "          -1.3934e+00, -3.0044e-01, -6.7471e-01,  5.8457e-01, -7.6047e-01,\n",
            "           6.0448e-02, -4.2249e-01, -1.0287e-01, -5.1515e-01,  5.2735e-01,\n",
            "           1.2225e-01],\n",
            "         [-1.4082e+00, -1.1119e+00,  3.4915e-02, -2.9473e-01,  1.0128e+00,\n",
            "           6.3857e-01, -5.8733e-01, -1.4819e-01, -1.0321e+00,  2.7393e+00,\n",
            "          -2.7134e-01,  8.6662e-01,  3.7701e-02, -1.6677e+00, -5.9282e-01,\n",
            "           2.9994e-01],\n",
            "         [-1.0124e-01,  1.5293e-01, -9.4317e-01,  8.2247e-01,  8.2523e-01,\n",
            "          -7.4834e-01, -1.6084e-01, -1.3586e+00,  3.4977e-01, -4.1790e-01,\n",
            "          -1.1978e-02,  9.9822e-01,  5.0018e-01,  5.8373e-01,  8.6015e-01,\n",
            "           3.8471e-01]]], grad_fn=<ViewBackward0>)\n",
            "tensor([[[ 0.9891,  0.8463,  0.4323,  0.1691, -1.5877, -0.0939,  0.4734,\n",
            "          -0.3382, -0.2781, -0.1380, -1.2280, -0.2923,  0.3122, -1.4082,\n",
            "           0.4685,  0.1283],\n",
            "         [ 1.5105,  1.1948,  0.4973,  2.1369,  0.4197, -0.9737, -1.3154,\n",
            "          -0.2085,  0.9277, -0.8306, -1.0014,  0.6860,  0.4023, -1.5626,\n",
            "           0.3105,  0.0827],\n",
            "         [-0.1379,  0.7181,  1.4695,  0.3873, -0.5997, -0.0356,  0.5513,\n",
            "          -0.2034, -0.7699,  0.7593, -0.2528,  0.1573, -0.2892,  0.3189,\n",
            "           0.2651, -0.0605],\n",
            "         [ 1.8922,  0.0449, -0.2549, -0.5804, -0.3934, -0.1626, -0.0491,\n",
            "           1.4862,  0.0340,  0.0877, -0.7220, -0.7505,  0.6399, -0.9807,\n",
            "          -0.1022, -0.3341],\n",
            "         [-1.0721, -0.1957,  0.9010, -0.4837,  0.2983, -0.6393, -0.7871,\n",
            "          -0.8476, -0.6424, -1.6009, -0.7812,  0.8101,  0.6218, -1.1636,\n",
            "           2.5503, -0.2486]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 2.5 스케일 점곱 방식의 어텐션\n",
        "\n",
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "  dim_k = querys.size(-1) #16\n",
        "  scores = querys @ keys.transpose(-2,-1) /sqrt(dim_k)\n",
        "  weights = F.softmax(scores, dim=-1)\n",
        "  return weights @ values"
      ],
      "metadata": {
        "id": "p68ZAdgYmIyZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 2.6 어텐션 연산의 입력과 출력\n",
        "print(\"원본 입력 형태: \", input_embeddings.shape)\n",
        "import torch\n",
        "import transformers\n",
        "after_attention_embeddings = compute_attention(querys, keys, values)\n",
        "\n",
        "print(\"어텐션 적용 후 형태: \", after_attention_embeddings.shape)\n",
        "\n",
        "# 원본 입력 형태:  torch.Size([1, 1, 5, 16])\n",
        "# 어텐션 적용 후 형태:  torch.Size([1, 5, 16])"
      ],
      "metadata": {
        "id": "VDj3RnMKmI1N",
        "outputId": "6ffeadc6-522a-4009-a560-a6ca1497c332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 입력 형태:  torch.Size([1, 1, 5, 16])\n",
            "어텐션 적용 후 형태:  torch.Size([1, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#예제 2.7 어텐션 연산을 수행하는 AttentionHead 클레스\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, token_embeded_dim, head_dim, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embeded_dim, head_dim) #  쿼리 벡터 생성을 위한 선형 층\n",
        "    self.weight_k = nn.Linear(token_embeded_dim, head_dim) #  키 벡터 생성을 위한 선형 층\n",
        "    self.weight_v = nn.Linear(token_embeded_dim, head_dim) #  값 벡터 생성을 위한 선형 층\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    outputs = compute_attention(\n",
        "        self.weight_q(querys),\n",
        "        self.weight_k(keys),\n",
        "        self.weight_v(values),\n",
        "        is_causal=self.is_causal\n",
        "    )\n",
        "    return outputs\n",
        "\n",
        "attention_head = AttentionHead(embedded_dim, embedded_dim)\n",
        "after_attention_embbedings = attention_head(input_embeddings, input_embeddings, input_embeddings)"
      ],
      "metadata": {
        "id": "tmWsR0o9mI4B"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_head)"
      ],
      "metadata": {
        "id": "UkSiBtQJkSxQ",
        "outputId": "9242d3a6-83fb-4630-dacf-3aa0f10ec12a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentionHead(\n",
            "  (weight_q): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (weight_k): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (weight_v): Linear(in_features=16, out_features=16, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(after_attention_embbedings)"
      ],
      "metadata": {
        "id": "OZKvKFoOkSzz",
        "outputId": "14021d79-608a-43ee-f337-c6d6c0a33f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.3197,  0.3011,  0.1351, -0.0755,  0.0653,  0.2729, -0.2246,\n",
            "            0.3369,  0.0901, -0.1661, -0.3104,  0.0417, -0.0414, -0.3447,\n",
            "            0.2401,  0.1230],\n",
            "          [-0.3747,  0.3352,  0.1125, -0.0608,  0.0503,  0.2187, -0.2044,\n",
            "            0.2766, -0.0044, -0.1856, -0.3022,  0.0147, -0.0613, -0.3669,\n",
            "            0.2010,  0.1616],\n",
            "          [-0.2649,  0.3924,  0.1248, -0.1189,  0.0384,  0.2022, -0.1723,\n",
            "            0.2180, -0.0122, -0.0596, -0.3699,  0.1277, -0.0149, -0.2565,\n",
            "            0.1836,  0.1869],\n",
            "          [-0.2916,  0.2133,  0.1664, -0.1265,  0.0589,  0.3189, -0.2263,\n",
            "            0.4053,  0.1717, -0.1647, -0.3186, -0.0025, -0.0381, -0.3235,\n",
            "            0.2861,  0.0466],\n",
            "          [-0.4935,  0.3335,  0.0784, -0.0309,  0.0688,  0.2022, -0.2163,\n",
            "            0.2436, -0.0340, -0.2554, -0.2800, -0.0092, -0.0835, -0.4443,\n",
            "            0.1949,  0.1787]]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.8 멀티 헤드 어텐션 구현\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
        "    super().__init()\n",
        "    self.n_head = n_head\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
        "    self.concat_linear= nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    B, T, C = querys.size()\n",
        "    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
        "    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
        "    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
        "    attentions = compute_attention(querys, keys, values, is_causal=self.is_causal)\n",
        "    attentions = attentions.transpose(1,2).contiguous().view(B, T, C)\n",
        "    outputs = self.concat_linear(attentions)\n",
        "    return outputs\n",
        "\n",
        "  n_head = 4\n",
        "  multihead_attention = MultiheadAttention(embedded_dim, embedded_dim, n_head)\n",
        "  after_attention_embeddings = multihead_attention(input_embeddings, input_embeddings, input_embeddings)\n",
        "  print(after_attention_embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "nwgoi_QTkS3N"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bnKw5ghKkS6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8h6Db792kS-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDyjDWZKkTAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKwNCeTykTDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}